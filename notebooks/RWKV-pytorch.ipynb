{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c982a411-7212-49a6-9d6b-50aad86efcda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import types, torch\n",
    "from torch.nn import functional as F\n",
    "from tokenizers import Tokenizer\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b31f2e-8fd1-469a-9ae6-b8612ef76873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41014b09-a9b5-4736-931c-74cc3e2e2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWKV_RNN(torch.jit.ScriptModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.eval() # set torch to inference mode\n",
    "        \n",
    "        w = torch.load(args.MODEL_NAME + '.pth', map_location=device)\n",
    "        for k in w.keys():\n",
    "            if      '.time_' in k: w[k] = w[k].squeeze()\n",
    "            if '.time_decay' in k: w[k] = -torch.exp(w[k].float()) # the real time decay is like e^{-e^x}\n",
    "            else: w[k] = w[k].float() # convert to f32 type\n",
    "        \n",
    "        self.w = types.SimpleNamespace() # set self.w from w\n",
    "        self.w.blocks = {}\n",
    "        for k in w.keys(): # example: \"blocks.0.att.time_first\" => self.w.blocks[0].att.time_first\n",
    "            parts = k.split('.')\n",
    "            last = parts.pop()\n",
    "            here = self.w\n",
    "            for p in parts:\n",
    "                if p.isdigit():\n",
    "                    p = int(p)\n",
    "                    if p not in here: here[p] = types.SimpleNamespace()\n",
    "                    here = here[p]\n",
    "                else:\n",
    "                    if not hasattr(here, p): setattr(here, p, types.SimpleNamespace())\n",
    "                    here = getattr(here, p)\n",
    "            setattr(here, last, w[k])\n",
    "\n",
    "    def layer_norm(self, x, w):\n",
    "        return F.layer_norm(x, (self.args.n_embd,), weight=w.weight, bias=w.bias)\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def channel_mixing(self, x, state, i:int, time_mix_k, time_mix_r, kw, vw, rw):\n",
    "        xk = x * time_mix_k + state[5*i+0] * (1 - time_mix_k)\n",
    "        xr = x * time_mix_r + state[5*i+0] * (1 - time_mix_r)\n",
    "        state[5*i+0] = x\n",
    "        r = torch.sigmoid(rw @ xr)\n",
    "        k = torch.square(torch.relu(kw @ xk)) # square relu, primer paper\n",
    "        return r * (vw @ k)\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def time_mixing(self, x, state, i:int, time_mix_k, time_mix_v, time_mix_r, time_first, time_decay, kw, vw, rw, ow):\n",
    "        xk = x * time_mix_k + state[5*i+1] * (1 - time_mix_k)\n",
    "        xv = x * time_mix_v + state[5*i+1] * (1 - time_mix_v)\n",
    "        xr = x * time_mix_r + state[5*i+1] * (1 - time_mix_r)\n",
    "        state[5*i+1] = x\n",
    "        r = torch.sigmoid(rw @ xr)\n",
    "        k = kw @ xk\n",
    "        v = vw @ xv\n",
    "        \n",
    "        aa = state[5*i+2]\n",
    "        bb = state[5*i+3]\n",
    "        pp = state[5*i+4]\n",
    "        ww = time_first + k\n",
    "        qq = torch.maximum(pp, ww)\n",
    "        e1 = torch.exp(pp - qq)\n",
    "        e2 = torch.exp(ww - qq)\n",
    "        a = e1 * aa + e2 * v\n",
    "        b = e1 * bb + e2\n",
    "        wkv = a / b\n",
    "        ww = pp + time_decay\n",
    "        qq = torch.maximum(ww, k)\n",
    "        e1 = torch.exp(ww - qq)\n",
    "        e2 = torch.exp(k - qq)\n",
    "        state[5*i+2] = e1 * aa + e2 * v\n",
    "        state[5*i+3] = e1 * bb + e2\n",
    "        state[5*i+4] = qq\n",
    "        return ow @ (r * wkv)\n",
    "\n",
    "    def forward(self, token, state):\n",
    "        with torch.no_grad():\n",
    "            if state == None:\n",
    "                state = torch.zeros(self.args.n_layer * 5, self.args.n_embd, device=device)\n",
    "                for i in range(self.args.n_layer): state[5*i+4] = -1e30 # -infinity\n",
    "            \n",
    "            x = self.w.emb.weight[token]\n",
    "            x = self.layer_norm(x, self.w.blocks[0].ln0)\n",
    "            for i in range(self.args.n_layer):\n",
    "                att = self.w.blocks[i].att\n",
    "                x = x + self.time_mixing(self.layer_norm(x, self.w.blocks[i].ln1), state, i, \n",
    "                    att.time_mix_k, att.time_mix_v, att.time_mix_r, att.time_first, att.time_decay, \n",
    "                    att.key.weight, att.value.weight, att.receptance.weight, att.output.weight)\n",
    "                ffn = self.w.blocks[i].ffn\n",
    "                x = x + self.channel_mixing(self.layer_norm(x, self.w.blocks[i].ln2), state, i, \n",
    "                    ffn.time_mix_k, ffn.time_mix_r, \n",
    "                    ffn.key.weight, ffn.value.weight, ffn.receptance.weight)\n",
    "            \n",
    "            x = self.w.head.weight @ self.layer_norm(x, self.w.ln_out)\n",
    "            return x.float(), state\n",
    "        \n",
    "    def sample_logits(self, logits, temperature=1.0, top_p=0.8, top_k=0):\n",
    "        top_k = int(top_k)\n",
    "        \n",
    "        probs = F.softmax(logits.float(), dim=-1)\n",
    "        sorted_ids = torch.argsort(probs)\n",
    "        sorted_probs = probs[sorted_ids]\n",
    "        sorted_probs = torch.flip(sorted_probs, dims=(0,))\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, 0)\n",
    "        cutoff = float(sorted_probs[torch.argmax((cumulative_probs > top_p).to(torch.long))])\n",
    "        probs[probs < cutoff] = 0\n",
    "        if top_k < len(probs) and top_k > 0:\n",
    "                probs[sorted_ids[:-top_k]] = 0\n",
    "        if temperature != 1.0:\n",
    "            probs = probs.pow(1.0 / temperature)\n",
    "        probs = probs / torch.sum(probs)\n",
    "        out = torch.multinomial(probs, num_samples=1)[0]\n",
    "        return int(out)\n",
    "    \n",
    "    def preprocess(self, token_ids, init_state=None):\n",
    "        for token_id in token_ids:\n",
    "            init_out, init_state = self.forward(token_id, init_state)\n",
    "        return init_out, init_state\n",
    "        \n",
    "    def generate_from_initial_state(self, init_out, init_state, temperature=0.5, top_p=0.85, max_num_tokens=100, top_k=0):\n",
    "        all_tokens = []\n",
    "        out_last = 0\n",
    "        out, state = init_out.clone(), init_state.clone()\n",
    "        for i in range(max_num_tokens):\n",
    "            token = self.sample_logits(out, temperature, top_p, top_k)\n",
    "            all_tokens += [token]\n",
    "            out, state = self.forward(token, state)\n",
    "\n",
    "        return all_tokens\n",
    "    \n",
    "    def generate(self, token_ids, temperature=0.5, top_p=0.85, max_num_tokens=100, top_k=0):\n",
    "        init_out, init_state = self.preprocess(token_ids)\n",
    "        return self.generate_with_initial_state(init_out, init_state, temperature, top_p, max_num_tokens, top_k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150dfb9f-a0b1-432a-94c9-97816b661dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model data/rwkv/RWKV-4-Pile-430M-20220808-8066 using cuda...\n",
      "Model loaded in 41.98 seconds\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(\"data/rwkv/20B_tokenizer.json\")\n",
    "\n",
    "args = types.SimpleNamespace()\n",
    "args.MODEL_NAME = 'data/rwkv/RWKV-4-Pile-430M-20220808-8066'\n",
    "args.n_layer = 24\n",
    "args.n_embd = 1024\n",
    "\n",
    "print(f'\\nLoading model {args.MODEL_NAME} using {device}...')\n",
    "start_time = time()\n",
    "model = RWKV_RNN(args)\n",
    "elapsed_time = time() - start_time\n",
    "print(f'Model loaded in {elapsed_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0422e36-0117-4ca7-8741-4f78b970ce55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------- Run number 1 -------\n",
      " In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. \n",
      "\n",
      "The dragons were found in a valley in the far west of Tibet, which is home to the largest population of Tibetan sheep. The dragons were discovered by a herding team of Chinese scientists who were searching for an area of unpopulated land that had been discovered by a team of Chinese scientists.\n",
      "\n",
      "The team of Chinese scientists discovered the dragons and discovered that there was a herd of dragons living in the valley. The dragons were discovered by a team of Chinese scientists who were searching for an\n",
      "\n",
      "\n",
      "------- Run number 2 -------\n",
      " In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. \n",
      "\n",
      "The scientists also found that the dragons were not only intelligent, but also very smart. The dragons had been studying the local population for a long time, and had even managed to find a way to communicate.\n",
      "\n",
      "The dragon population in Tibet is considered to be the world’s largest herd of wild animals.\n",
      "\n",
      "The scientists found that the dragons were not only intelligent, but also very smart. The dragons were also extremely intelligent, and were able to communicate with one another.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------- Run number 3 -------\n",
      " In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. \n",
      "\n",
      "The scientists believe that the dragons were trying to communicate with humans in order to communicate with them.\n",
      "\n",
      "The dragons are believed to be able to communicate with humans through their vocalizations, which were found in the cave.\n",
      "\n",
      "The dragons were found in a cave in the Himalayas, and they were discovered after a Tibetan monk found a dragon which was living in a cave in the valley.\n",
      "\n",
      "The dragons were found to be living in a cave in the valley, in Tibet\n",
      "\n",
      "\n",
      "------- Run number 4 -------\n",
      " In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. \n",
      "\n",
      "The dragons were kept in the cave in the hope of finding a new home. But as the dragons grew, they became too large for the cave, and the dragons were forced to move to a new area. The dragons were eventually found in a cave in the Himalayas, and were found to be the size of a small house.\n",
      "\n",
      "The dragons were so large that they could not have been domesticated, and the researchers were able to find them in a mountainous area in\n",
      "\n",
      "\n",
      "------- Run number 5 -------\n",
      " In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. \n",
      "\n",
      "The dragons were found in the early morning hours of May 25, when the researchers were on a routine patrol in the mountainous region of Tibet.\n",
      "\n",
      "“We thought that if we could find a dragon, it would have some kind of communication with the dragon, so we sent out a team of scientists to the valley. The dragon was very aggressive,” said Cheng, a PhD student at the University of Technology, Sydney, who was not involved in the research.\n",
      "\n",
      "“It was\n"
     ]
    }
   ],
   "source": [
    "context = \"In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n",
    "\n",
    "token_ids = tokenizer.encode(context).ids\n",
    "init_out, init_state = model.preprocess(token_ids)\n",
    "for i in range(5):\n",
    "    generated = tokenizer.decode(model.generate_from_initial_state(init_out, init_state))\n",
    "    print(f'\\n\\n------- Run number {i+1} -------\\n',context, generated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f7a64-82e6-412c-9c62-b5986a2a32b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
